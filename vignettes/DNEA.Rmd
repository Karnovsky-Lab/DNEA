---
title: "Differential Network Expression Analysis for Metabolomics Data"
subtitle: "A Data-Driven Approach to Network Analysis"
author: "Christopher P. Patsalis"
date: "`r format(Sys.Date(), '%m/%d/%Y')`"
abstract: >
  Advancements in analytical methods, such as Liquid Chromatography-Mass Spectrometry (LC-MS), have enabled the high-throughput identification of hundreds to thousands of metabolites in biological samples, creating larger, more complex datasets that need to be analyzed. Pathway enrichment analysis is commonly used to identify the metabolic mechanism(s) underlying a disease state. However, there are several challenges present in analyzing metabolomics and lipidomics data sets using traditional bioinformatics tools. These conventional methods, like gene set enrichment analysis (@GSEA2005) and Gene Ontology (@GO2009), translate poorly to metabolomics and lipidomics data due to de novo identification of the metabolome making it difficult to annotate pathway databases. Further, metabolomics studies often identify exogenous compounds that cannot be mapped to a human pathway. 
  
  
  To combat this, our group developed ***The Differential Network Expression Analysis (DNEA)*** algorithm outlined in @DNEA2019 and later implemented in the Filigree java-application presented in @Filigree2021. DNEA is a data-driven network analysis tool for biologial data that utilizes gaussian graphical models (GGM) to jointly estimate the biological networks of two conditions. Data-driven approaches to network analysis of metabolomics and lipidomics data has been difficult due to an abundance of samples in comparison to the number of features in the average -omics study, also known as the ***p >> n*** problem. R1 regularization, in combination with a novel method for stability selection, allow us to overcome this by only keeping important metabolites in the model. This approach provides more robust results and enables the identification of true metabolite-metabolite interactions otherwise lost by reference-based methods. Networks constructed using DNEA can be clustered to identify metabolic modules, or sub networks, within the data and subsequently test them for enrichment across experimental conditions using the [netgsa R package](https://cran.rstudio.com/web/packages/netgsa/index.html). By using a data-driven approach to define the metabolic pathways, we improve the accuracy of pathway analysis.
  
  
  The DNEA R package is the latest implementation of the algorithm and implements several enhancements to the workflow. GGM's require considerably more compute power and, as biological datasets grow, the available resources on the typical personal computer can be constraining. DNEA is designed to work on high-performance or cloud-computing machines to utilize all of the available computing cores through parallelization and keep the memory footprint at a minimum. The algorithm has also been modularized into several key steps. The user now has more control over model tuning and edge inclusion through filtering of the resulting networks based on the strength of the metabolite-metabolite associations. This enables tailoring of the analysis to their data. While we have only tested DNEA on metabolomics and lipidomics data, theoretically it is applicable to any continuous data type that is aproximately normal (i.e. proteomics expression data).
output:
  BiocStyle::pdf_document:
    toc: true
bibliography: DNEA_bibliography.bib
vignette: >
  %\VignetteIndexEntry{DNEA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r, environment setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  prompt = TRUE
)

library(dplyr)
library(knitr)
library(kableExtra)
```
# Downloading DNEA
The DNEA R package is currrently available on the  [Karnovsky Lab github](https://www.github.com/Karnovsky-Lab/DNEA). Any troubleshooting or issues with the software can be reported there. The package can be installed using devtools. BiocManager is also needed to install several package dependencies stored on Bioconductor. Let's download those packages if necessary, and load them into R.

```{r, install , eval=FALSE}
#install BiocManager if not already
if(!nzchar(system.file(package = "BiocManager"))){
  install.packages("BiocManager")
}

#install devtools if not already
if(!nzchar(system.file(package = "devtools"))){
  install.packages("devtools")
}

#load packages
library(devtools)
library(BiocManager)
```
Now that those are loaded, we can install DNEA and its dependencies.
```{r, install DNEA, eval=FALSE}
#install dependency packages
BiocManager::install(c("BiocParallel","dplyr","gdata","glasso","igraph",
                       "janitor","Matrix","stringr","netgsa"))

#install DNEA
devtools::install_github("Karnovsky-Lab/DNEA")
```
# Basic DNEA workflow
This package is the most customizable implementation of DNEA to date, allowing the user to create the best analysis for their data. This vignette aims to walk you through a typical DNEA workflow, and describe the parameters that may be modified in common use cases.  

We will use the metabolomics expression data derived from The Environmental Determinants of Type I Diabetes in the Young (TEDDY) clinical trial (@TEDDY) to demonstrate DNEA. TEDDY Is a prospective case-control study that seeks to better understand environmental causes of Type I diabetes. High-risk infants, as defined by the presence of HLA-DR and HLA-DQ genotypes, were followed every 6 months and samples were collected until development of type I diabetes or their 15 birthday, whichever occurred first. The study consisted of two conditions, Islet auto-antibody (IA) case vs. control and Type I Diabetes (T1D) case vs. control, but we will focus on the latter here. The data was originally accessed from Metaboolomics Workbench under Project ID: [PR000950](https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Project&ProjectID=PR000950). The T1D arm contains 3525 samples from 440 subjects, 50 of which developed Type I Diabetes. The example data contains the samples from those 50 subjects on the date of diagnosis as well as samples from the 322 control subjects closest to that day. It is stored in the package, and instructions on how to access the data are below. Please visit the Metabolomics Workbench for more information about the data set.

# Input data
There are 3 inputs required to initiate the DNEA workflow: a character string corresponding to the experiment name indicated by `project_name`, the expression data indicated by `expression_data`, and the experimental groups indicated by `group_labels`.  

## expression_data
The `expression_data` should be an *m x n* numeric matrix wherein the metabolites (or lipids, proteins, etc.) each have a row, and the samples each have a column. DNEA jointly estimates the biological network for each experimental condition using Guassian Graphical Model's (GGM), so it is important that the data for each group is approximately normal. To that end, DNEA log transforms and auto-scales the expression data for each group independently when the workflow is initiated. The input should be raw peak intensities or metabolite concentrations. Differential expression analysis is an important part of pathway enrichment analysis downstream, and it cannot be performed after the experimental groups have been normalized separately since the mean expression for each metabolite within each experimental group is then centered around zero. ***NOTE: It is acceptable to adjust the entire data set (i.e. both experimental groups together) for batch effect or confounding variables prior to analysis.***


Let's load the example data stored inside the DNEA package. It is a numeric expression matrix where the metabolites are in rows and the samples are in columns. The peak intensity data has been adjusted for age and sex to remove those confounding variables, ***but it has not been log-transformed or scaled.***
```{r, load example data, eval=1:5, echo=TRUE}
#first load DNEA into R
library(DNEA)

#load the example data
data("TEDDY")
```
```{r, TEDDY examine echo, eval=FALSE, echo=TRUE}
TEDDY[1:10, 1:4]
```
```{r, TEDDY examine, echo=FALSE}
TEDDY_table <- knitr::kable(TEDDY[1:10, 1:4])
kable_styling(TEDDY_table, "striped", position = "left", full_width = TRUE)

#remove table
rm(TEDDY_table)
```
## group_labels
The `group_labels` should be a vector of factor elements that correspond to the experimental condition of each sample. Each element should be named for its corresponding sample, and the order should match the order of the samples in *expression_data*. We can create the `group_labels` object for the *TEDDY* data using the metadata, *T1Dmeta*, stored in DNEA. Each row is a different sample, and each column is a different variable. We need the experimental conditions for each sample in the "group" column and the names for each sample, which are the row names of the data frame. More information about the available metadata can be found in the *T1Dmeta* documentation.
```{r, load example metadata, eval=TRUE}
#load T1Dmeta
data("T1Dmeta")

unique(T1Dmeta$group)
```
```{r, T1Dmeta echo, eval=FALSE, echo=TRUE}
#View the metadata
T1Dmeta[1:10, c(1,5,6,7)]
```
```{r, T1Dmeta examine, echo=FALSE}
T1Dmeta_table <- knitr::kable(T1Dmeta[1:10, c(1,5,6,7)])
kable_styling(T1Dmeta_table, "striped", position = "left", full_width = TRUE)

#remove table
rm(T1Dmeta_table)
```
DNEA is designed to jointly estimate biological networks from only ***TWO*** experimental conditions. If you have one experimental condition, you should consider using another tool we developed, [CorrelationCalculator](http://metscape.ncibi.org/calculator.html). Our experimental condition has two possible values: "DM:control" and "DM:case". Now we can re-order the metadata to match the sample order of the expression data, and save the group labels as a new vector element. Finally, we can convert this character vector into factors. We will specify that "DM:control" is the reference.
```{r, create group_labels, eval=TRUE}
#re-order the metadata to match the sample order of expression_data
T1Dmeta <- T1Dmeta[colnames(TEDDY),]

#save the group column to be used as group_labels
group_labels <- T1Dmeta$group

#name each element for its corresponding sample
names(group_labels) <- rownames(T1Dmeta)

#convert to factor
group_labels <- factor(group_labels, levels = c("DM:control", "DM:case"))
```

# STEP 1: Data pre-processing and feature aggregation
## Data pre-processing
DNEA is an object-oriented workflow built around a custom s4 object, `DNEAresults.` We provide an initiator function that encompasses several necessary steps to begin DNEA. First, the data is restructured for input into a `DNEAresults` object, and differential expression (DE) analysis is performed after log-transforming the expression values. The data is then split by experimental condition and auto-scaled. An eigendecomposition is performed to check the minimum eigenvalue and condition number of the correlation matrix for the whole data set, as well as each experimental group individually.
```{r, start DNEA, eval=TRUE}
#initiate DNEAresults object
TEDDYdat <- createDNEAobject(project_name = "TEDDYmetabolomics",
                             expression_data = TEDDY,
                             group_labels = group_labels)
```
Eigenvalues close to or below zero, or similarly extremely large condition numbers, represent instability in the data set and triggers a warning that recommends you aggregate highly correlated metabolites prior to analysis. As you can see above, the TEDDY data set has a negative eigenvalue for the "DM:case" experimental group, triggering a warning.

**WHAT CAUSED THIS WARNING?**  

There are a number of reasons that instability may occur within a data set; Highly correlated features is one of the most common causes in expression data. We can view this by plotting a heatmap of the pearson correlation matrix for the "DM:case" data. The log-scaled data is accessed using the `expressionData()` function. We filter to only "DM:case" samples by indexing the data using the group labels accessed via `networkGroupIDs()`.
```{r, correlation heatmap, eval=TRUE, message=FALSE}
#load the pheatmap and Hmisc packages
library(pheatmap)
library(Hmisc)

#grab the data from the DNEAresults object
expr_dat <- expressionData(TEDDYdat, normalized = TRUE)

#only keep the "DM:case" data
expr_dat <- expr_dat[, networkGroupIDs(TEDDYdat) == "DM:case"]

#create a pearson correlation matrix - data should be transposed first so features are in columns
cor_dat <- Hmisc::rcorr(t(expr_dat), type = "pearson")$r

#cluster the correlations and reorder correlation matrix to better visualize
  dd <- as.dist((1-cor_dat)/2)
  hc <- hclust(dd)
  cor_dat <-cor_dat[hc$order, hc$order]
  
  #create pheatmap
  pheatmap(cor_dat, cluster_rows = FALSE,cluster_cols = FALSE,
         legend = TRUE,annotation_legend = FALSE,
         labels_row = '',labels_col = '',
         main = 'Feature correlations in DM:case group'
)

```


There are pockets of red and blue indicating highly correlated metabolites in this data set that may be causing the instability. The regularization steps employed in DNEA will correct this issue without additional user-intervention, however, it may be desirable to preemptively address this by aggregating highly-correlated metabolites.  


**WHY SHOULD WE AGGREGATE?**  


Aggregating highly-correlated metabolites into a single group prior to analysis may be of benefit for several reasons, particularly if the dataset contains many features of the same class of compounds (ie. fatty acids, carnitines, etc.). Doing so retains signal from all of the metabolites, as opposed to regularization arbitrarily choosing one as representation and removing some or all of the correlated correlates. This gives the user more control over network construction and simplifies the resulting networks.

We can view information about our analysis by calling *TEDDYdat*. We can access the differential expression results in the node list using the `nodeList()` function.
```{r, DNEA summary, eval=1:4, echo=TRUE}
#show summary of DNEAresults object
TEDDYdat
```
```{r, node list examine echo, eval=FALSE, echo=TRUE}
#access node list
nodeList(TEDDYdat)[1:5,]
```
```{r, node list examine, echo=FALSE}
nodelist_table <- knitr::kable(nodeList(TEDDYdat)[1:5,])
kable_styling(nodelist_table, "striped", position = "left", full_width = TRUE)

#remove table
rm(nodelist_table)
```
<div align = "center"><font size="4" color = red>To skip optional feature aggregation, </font></div><br>
<div align = "center"><font size="4" color = red> move on to the next section: [STEP 2: Model Tuning] </font></div><br>

## Feature Aggregation
GGM's require considerable computational power. Adding additional samples to your data set will have marginal effects on the memory and processing time required for analysis, however, as the number of features grows in your data set the run time will increase dramatically. The ability to parallelize tasks on a high-performance machine (cluster or cloud resource) to perform stability selection helps tremendously with this issue. Even so, a user may still find themselves constrained by the resources available to them. Moreover, the data set may contain many compounds of the same class that are highly correlated, and/or network resolution at the individual molecule level is not needed (i.e. fatty acids that vary by only a few bonds in a lipidomics study). In these cases, aggregating highly correlated metabolites into a single node decreases the complexity of the analysis without losing critical network information. Less compute resources are required for the analysis and the resulting networks are more interpretable.


We have implemented an updated node aggregating algorithm similar to the one described in @Filigree2021. The `aggregateFeatures()` function makes available the **correlation-based**, **knowledge-based**, and **hybrid** collapsing methods from Filigree. The main difference being that the user now specifies a pearson correlation threshold by which to collapse features with an absolute correlation above said value. The function then re-runs `createDNEAobject()` to create a `collapsed_DNEAresults` object with the new data.  


To perform feature aggregation, we need to provide `aggregateFeatures()` the `DNEAresults` object and a character string corresponding to the desired aggregation method. If the "correlation" or "hybrid" method is chosen, we specify the `correlation_threshold` parameter wherein metabolites with correlations stronger than this threshold are aggregated into a single node. If the "knowledge" or "hybrid" method is chosen, we need to provide a data frame for `feature_groups` to specify the class of each metabolite in our data set. Metabolite aggregation is then performed within each class. More information about the three methods as well as best use cases can be found in the function documentation and in @Filigree2021.  


For demonstration purposes, we are going to use the "hybrid" approach for feature aggregation. The `feature_groups` input should be a data frame with the original metabolite names as the first column column and also the row names. The class labels should be in the second column. Independent metabolites can retain their name as its class label. Let's create the `feature_groups` data frame, and group the branched chain amino acids into one class and all other acids into another.
```{r, setup input for aggregateFeatures, eval=TRUE}
#save metabolite names
metab_names <- rownames(expressionData(TEDDYdat, normalized = FALSE))

#create feature_group data.frame
TEDDY_groups <- data.frame(features = metab_names,
                           groups = metab_names,
                           row.names = metab_names)

#create labels
TEDDY_groups$groups[TEDDY_groups$groups %in% c("isoleucine",
                                               "leucine",
                                               "valine")] <- "BCAAs"
TEDDY_groups$groups[grepl("acid",
                          TEDDY_groups$groups)] <- "fatty_acids"


```
```{r, TEDDY groups echo, eval=FALSE, echo=TRUE}
#take a look at the group labels
TEDDY_groups[1:10, ]
```
```{r, node collapse groups, echo=FALSE}
netGSA_table <- knitr::kable(TEDDY_groups[1:10, ])
kable_styling(netGSA_table, "striped", position = "left", full_width = TRUE)
```

For the "hybrid" method we also have to provide a `correlation_threshold` value. The algorithm will only aggregate metabolites within the same class that have a stronger correlation than the value provided with this method.
```{r, run aggregateFeatures, eval=TRUE}
#perform feature collapsing
collapsed_TEDDY <- aggregateFeatures(object = TEDDYdat,
                                  method = "hybrid",
                                  correlation_threshold = 0.7,
                                  feature_groups = TEDDY_groups)

collapsed_TEDDY
```
We have reduced our total number of features from 134 to 130. To use the aggregated data, we would continue our analysis with the *collapsed_TEDDY* object. However, this data set is a curated list of diverse metabolites, so high correlations are not likely a result of chemical similarity. We do not want to erroneously combine disparate features, so it is better to continue without aggregating. Therefore, we will use the *TEDDYdat* object for the rest of the analysis.

## [OPTIONAL] Custom-Normalized Data Input
The expression data is log transformed and auto-scaled by DNEA upon initiation of the workflow with `createDNEAobject()`. However, varying normalization methods are used for metabolomics and lipidomics data (e.g. auto-scaling, median-scaling, quantile-normalization, RUV2) that may affect correlation-based studies differently. If you have a preferred normalization method, we provide an additional helper function, `addExpressionData()`, to input custom-normalized data into the object for analysis. There are two criteria that need to be met by normalized data:  

**1.** Intensities/concentrations of each metabolite are approximately normally distributed  

**2.** The data for each experimental condition is normalized separately  

As an example, let's median-scale the TEDDY data and insert it into the *TEDDYdat* object.
```{r, addExpressionData, eval=TRUE}
#make sure metadata and expression data are in same order
T1Dmeta <- T1Dmeta[colnames(TEDDY),]

dat <- list('DM:control' = TEDDY[,T1Dmeta$group == "DM:control"],
            'DM:case' = TEDDY[,T1Dmeta$group == "DM:case"])

#log-transform and median center the expression data without scaling
newdat <- NULL
for(cond in dat){

  #median scale data
  cond_scaled <- apply(cond, 1, function(x) (x - median(x, na.rm = TRUE)) /
                         max(abs(range(x, na.rm = TRUE) - median(x, na.rm = TRUE))))
  
  #add condition data to output
  newdat <- cbind(newdat, cond)
}

#add row names and column names
colnames(newdat) <- colnames(TEDDY)
rownames(newdat) <- rownames(TEDDY)

#reorder to match dnw
newdat <- newdat[featureNames(TEDDYdat), sampleNames(TEDDYdat)]

#add data
TEDDYcustomDat <- addExpressionData(object = TEDDYdat, data = newdat)
```
This function inserts the provided data into the `scaled_expression_data` list element in the assays slot and moves the log-transformed and auto-scaled expression data created by DNEA to the `DNEA_scaled_data` list element. We are going to proceed with the *TEDDYdat* object and the log-scaled data inherent to DNEA in the next step.

# STEP 2: Model Tuning
Model tuning utilizes two regularization methods: $\lambda$ tuning via Bayesian-information criterion (BIC), and stability selection. This step allows us to analyze data sets with less samples than the number of metabolites and is critical for DNEA. When **the number of samples approaches or exceeds the number of features**, regularization in the model is not strictly necessary and you may proceed without this step. This allows users with very large data sets to create networks from a simplified glasso model without regularization. If tuning is not performed, the $\lambda$ value defaults to   

<div align = "center">$\lambda = \sqrt{ \ln (num. features) / num. samples}$</div> 
and all of the selection probabilities are set to 1. However, we highly recommend performing this step irrespective. Regularization adds sparsity to the resulting networks and improves model specificity by removing weak or potentially false positive edges.

## $\lambda$ tuning via BIC
The `BICtune()` function optimizes the $\lambda$ parameter by calculating a BIC score and likelihood value for every tested $\lambda$, as described in @BIC2011. We minimized the necessary computation to optimize the parameter by selecting values surrounding the asymptotically valid $\lambda$ for data sets with many samples and many features, following the equation:  

<div align = "center">$\lambda = c \sqrt{ \ln (num. features) / num. samples}$</div> 
where c takes on 15 evenly spaced values between 0.01 and 0.3. The user can also opt to provide a range of $\lambda$ values to test using the `lambda_values` parameter.


Each $\lambda$ value tested can be run embarrassingly parallel using the BiocParallel package. Creating forked processes via the `MulticoreParam()` function is the most resource efficient parallelization method.
```{r, BiocParallel MulticoreParam, eval=FALSE}
#load in BiocParallel
library(BiocParallel)

#example of forked workers
BPPARAM <- BiocParallel::MulticoreParam(workers = 2, RNGseed = 417)
```
However, there are several limitations:  
**1.** Forked processes are not available on the Windows operating system.  
**2.** Forked processes can be unstable in Rstudio.  

For the sake of this vignette we're going to create parallel sockets. We only need to create the workers once and they can be called for each parallel process. Since `stabilitySelection()` uses random number generation to randomly sample the expression data in each replicate, we will have to set the seed now so that the results are reproducible.  
```{r, BiocParallel, eval=TRUE}
#load in BiocParallel
library(BiocParallel)

#create parallel sockets
BPPARAM <- BiocParallel::SnowParam(workers = 2, RNGseed = 417)
```
Now that we have our workers set up, we can optimize $\lambda$. If you are performing this analysis on a personal computer, we provided a real-time progress bar to keep you updated. You can set the `verbose` parameter to `FALSE` to silence it.
```{r, BICtune, eval=TRUE}

#optimize lambda
TEDDYdat <- BICtune(TEDDYdat, BPPARAM = BPPARAM, verbose = FALSE)
```
Our tuned lambda value is 0.03891489239239. This was selected by minimizing the BIC score. To illustrate BIC optimization, we can plot the BIC scores as a function of $\lambda$
```{r, BIC plot, eval=TRUE}
#load ggplot2
library(ggplot2)
#create data frame of values
BICtuneData <- rbind(data.frame(lambda = unlist(lambdas2Test(TEDDYdat)),
                          value = vapply(BICscores(TEDDYdat), function(x) x$BIC, double(1)),
                          score = rep("BIC", length(lambdas2Test(TEDDYdat)))),
                     data.frame(lambda = unlist(lambdas2Test(TEDDYdat)),
                          value = vapply(BICscores(TEDDYdat), function(x) x$likelihood, double(1)),
                          score = rep("likelihood", length(lambdas2Test(TEDDYdat)))))

#create plot
ggplot(data = BICtuneData, aes(x = lambda, y = value)) + 
  geom_line(aes(linetype = score)) + 
  geom_point(aes(shape = score)) +
  geom_vline(xintercept = optimizedLambda(TEDDYdat), color = "red") +
  ggtitle("BIC and Likelihood Scores with Respect to Lambda")

```

## Stability Selection
The `stabilitySelection()` function calculates selection probabilities, or the estimated probability that an edge is identified in a randomly sampled subset of the input data, using the method outlined in @DNEA2019. The selection probability for each metabolite-metabolite interaction is then used to modify the lambda parameter following the equation:  

<div align = "center">$\rho = \lambda*(1 / (1e-04 + selection.probability))$</div>
To perform stability selection, we need to pass `stabilitySelection()` the `DNEAresults` object, the number of replicates to perform, the BiocParallel object specifying our workers for parallelization, and whether or not additional sub sampling of the data should be performed. We recommend setting `nreps = 500`, which is the default value. Stability selection *without* additional sub-sampling randomly samples 50% of each group (without replacement), creating two evenly sampled data sets and fitting a GGM to both. This means that at the default `nreps = 500`, 1000 replicates are actually performed.


When the sample groups are very unbalanced, the selection probabilities from stability selection  strongly favor the larger group resulting in over-representation. We combat this by employing a sub-sampling protocol during stability selection that was first introduced in @Filigree2021 by setting `subSample = TRUE`. This method ensures that each group is equally represented in stability selection. Since nearly all of the data for the smaller group is used *with* additional sub-sampling, only one model is fit per replicate. When utilizing the sub-sampling protocol, `nreps` should be set to 1000.

If you elect to optimize the $\lambda$ parameter using a different method than `BICtune()`, you can specify the value to use with the `optimal_lambda` parameter. Here, we will set `nreps` to 500 and `subSample = FALSE`. This is the most computationally expensive step in the algorithm. It takes ~28 minutes using 4 cores on a 2.5 GHz Quad-Core Intel Core i7 processor. If you're running DNEA locally, set `verbose = TRUE` to get real-time updates on the progress of stability selection.
```{r, load processed data, include=FALSE}

#load processed results since 500 reps would take ~28 min
data("dnw")
TEDDYdat <- dnw
rm(dnw)
```

```{r, stabsel show, eval = FALSE, echo = TRUE, strip.white=TRUE}
#perform stability selection
TEDDYdat <- stabilitySelection(TEDDYdat,
                               subSample = FALSE,
                               nreps = 500,
                               BPPARAM = BPPARAM,
                               verbose = FALSE)
```
```{r, stabsel code, eval=3, echo=FALSE, strip.white=TRUE, prompt=FALSE}
#return stabsel messages
message("Using Lambda hyper-parameter: ", optimizedLambda(TEDDYdat), "!\n",
        "stabilitySelection will be performed with 500 replicates!")
```

# Step 3: Constructing the Networks and Consensus Clustering
Now that we have optimized the input parameters, we can jointly estimate the biological networks. We will construct a glasso model to calculate the partial correlation value for each metabolite-metabolite interaction. We can then perform consensus clustering to identify metabolic modules, or sub networks, within the larger experimental group networks.

## Constructing the Networks
We provide a function, called `getNetworks()`, to perform joint-estimation of the biological networks. The necessary input parameters were optimized during model tuning and are already stored in the *TEDDYdat* object. We just need to pass it to the `getNetworks()` function.
```{r, getNetworks, eval = TRUE}
#jointly estimate the biological networks
TEDDYdat <- getNetworks(TEDDYdat)
```
The TEDDY data set contains 134 metabolites, so a completely dense network has $134*134=17956$ possible edges. For obvious reasons, we do not expect every feature to be connected with every other feature and the regularization steps we performed in [Step 2: Model Optimization] have removed the non-connections. There are 1656 total edges remaining in the networks: 373 edges specific to the "DM:control" network, 871 edges specific to the "DM:case" network, and 412 edges identified in both. We can access the edge list using the `edgeList()` function.
```{r edgeList, eval=FALSE, echo=TRUE}
#save edge list to new object
edge_list <- edgeList(TEDDYdat)

#access the edge list
edge_list[1:10,]
```

```{r edge table, echo=FALSE}
#save edge list to new object
edge_list <- edgeList(TEDDYdat)

#create kable
edge_table <- knitr::kable(edge_list[1:10,])
kable_styling(edge_table, "striped", position = "left", full_width = TRUE)

#clean up space
rm(edge_table)
```

Some of the edges are extremely weak and it would best suit our analysis to remove them. Weak interactions are not as interesting from a biological perspective, and they disrupt the performance of consensus clustering resulting in a small number of large networks. They may also clutter the network, making it difficult to derive meaningful biological insight. The function `filterNetworks()` can be used to remove weak edges. Filtering edges is accomplished by either providing a partial correlation value using the `pcor` parameter, or a percentage using the `top_percent_edges` parameter. If `pcor` is provided, all edges with an absolute partial correlation less than the specified value are removed. If `top_percent_edges` is provided, only the strongest *X%* of edges in the network are retained.  


683 of the edges in "DM:control" network and 1103 of the edges in the "DM:case" network have a partial correlation value between -0.166 and 0.166.  We will set 0.166 as the absolute threshold.
```{r, filterNetworks, eval=TRUE}
##edges between -0.166 and 0.166
#control network
sum(abs(edge_list$pcor.0[edge_list$pcor.0 != 0]) < 0.166)

#case network
sum(abs(edge_list$pcor.1[edge_list$pcor.1 != 0]) < 0.166)
#filter networks based on an absolute threshold of pcor = 0.166
TEDDYdat <- filterNetworks(TEDDYdat, pcor = 0.166)
```
After filtering, we now have 236 total edges in the networks: 56 edges specific to the "DM:control" network, 134 edges specific to the "DM:case" network, and 46 edges identified in both.

## Consensus Clustering
The consensus clustering algorithm described in @DNEA2019 and employed here utilizes 7 network clustering algorithms implemented in the [igraph R package](https://r.igraph.org/). It works by clustering the data to identify sub networks of highly inter-connected metabolites within the networks. Consensus clustering is performed iteratively until agreement on the resulting sub network membership is reached. This enables the use of data-driven pathway enrichment analysis downstream to identify sub networks that are differentially enriched across the two experimental conditions.  


Consensus clustering is performed by passing the `DNEAresults` object to `clusterNet()`. You may opt to specify the `tau` parameter, which corresponds to the percent agreement threshold (i.e. *tau%* of the clustering algorithms must agree on the metabolite membership within a sub network). `tau` can range from 0.5-1. The default value is 0.5, or 4 of the 7 algorithms must be in agreement. Increasing the value of tau will increase the specificity of the analysis, and therefore decrease the number and size of the resulting sub networks.   


Several of the clustering methods utilize random number generation. Since `clusterNet()` does not use the BiocParallel framework, we need to set the seed in native R to ensure reproducibility of our results.
```{r, consensus cluster, eval = TRUE, warning=FALSE}
#set the seed
set.seed(417)

#perform consensus clustering
TEDDYdat <- clusterNet(TEDDYdat, tau = 0.5,
                       max_iterations = 5,
                       verbose = FALSE)
```
```{r, CCsummary show, eval = FALSE}
#view subnetwork summary
CCsummary(TEDDYdat)
```
We can access information about the clustering results with `CCsummary()`. The summary shows you the number of nodes and edges per network as well as how many were differentially expressed, respectively. All except one of the 134 metabolites clustered into the 13 sub networks.
```{r, CCsummary kable, echo=FALSE}
clust_table <- knitr::kable(CCsummary(TEDDYdat))
kable_styling(clust_table, "striped", position = "left", full_width = TRUE)
```


# Step 4: Pathway Enrichment via NetGSA and Network Visualization
Now that we have constructed our biological networks and identified metabolic modules within them, we can perform additional analyses to help us derive biological insight from our data. Two common analyses are pathway enrichment and visualization.

## Pathway Enrichment via NetGSA
The data-driven approach to network construction overcomes challenges faced in more traditional pathway analyses of metabolomics and lipidomics data by using the correlation structure of the data to define metabolic modules. We can then test them for enrichment across the experimental condition using [netGSA](https://cran.rstudio.com/web/packages/netgsa/index.html). DNEA contains a wrapper function for the netgsa algorithm, `runNetGSA()`. Everything we need for the analysis is passed to the function with *TEDDYdat*. We can access the results using the `netGSAresults()` function.
```{r, runNetGSA, eval = 1:2, echo=TRUE}
#perform pathway enrichment using netgsa
TEDDYdat <- runNetGSA(TEDDYdat, min_size = 5)
```
```{r, netGSAresults show, eval = FALSE}
#access netGSA results
netGSAresults(TEDDYdat)
```
```{r, netGSAresults kable, echo=FALSE}
netGSA_table <- knitr::kable(netGSAresults(TEDDYdat))
kable_styling(netGSA_table, "striped", position = "left", full_width = TRUE)
```
Of the 13 sub networks defined in our data, 9 contained 5 or more features and were tested for differential enrichment. Two of the 9 sub networks are significantly enriched across diabetes status in the TEDDY data. <font color = red>***NOTE:*** *The sub networks have been reordered by their false-discovery rate calculated during enrichment analysis, so the sub network numbering may look different after pathway analysis as compared to prior.*</font>

## Network Visualization
There are several common tools for visualizing biological networks, three of the most common being:  

1. [Cytoscape](https://cytoscape.org/)  
2. [Metscape](http://metscape.med.umich.edu/)  
3. [igraph package in R](https://r.igraph.org/)  

DNEA provides functionality that makes using all three easy.  


<font size="3">**Visualizing Networks using DNEA and igraph**</font>  

The igraph R package is commonly used to visualize networks due to its customization. DNEA contains a function, `plotNetworks()` that is built on igraph. This function provides the user an easy way to visualize the constructed networks and utilize the features available in the igraph package. Edges specific to group 1, in our case "DM:control", are colored <font color = green>**green**</font> and edges specific to group 2, or "DM:case", are colored <font color = red>**red**</font>. Common edges are **black**, and DE nodes are colored <font color = purple>**purple**</font>.  


There are two parameters that are required in addition to the `DNEAresults` object: the `type`, and the `subtype`. When the `type` parameter is set to "group_networks" we can plot the networks for either of the experimental groups. We do so by providing its label (i.e. "DM:control" or "DM:case") to the `subtype` parameter, or "All" to plot both networks. 
```{r, plotNetworks biological networks}
#names of our experimental conditions
networkGroups(TEDDYdat)

#create side by side plots
par(mfrow = c(1,3))

#plot networks
plotNetworks(TEDDYdat, 
             type = "group_networks",
             subtype = "DM:control",
             main = "DM:control Network")
plotNetworks(TEDDYdat, 
             type = "group_networks",
             subtype = "All",
             main = "Joint Network")
plotNetworks(TEDDYdat, 
             type = "group_networks", 
             subtype = "DM:case",
             main = "DM:case Network")
```


We can also plot the sub networks by setting `type` to "subnetworks" and specifying which sub network to plot. Sub networks 1 and 2 are both differentially enriched, so let's plot them by setting `subtype = 1` and `subtype = 2`, respectively. We can change the layout to a circle by providing the igraph `layout_in_circle()` function to the `layout_func` parameter. We will need to load the igraph package into our environment first to do so. More information about customizing network figures using `plotNetworks()` can be found in the function documentation.
```{r, plotNetworks sub networks, message=FALSE}
#load igraph
library(igraph)

#create side by side plots
par(mfrow = c(1,2))

#plot subnetworks
plotNetworks(TEDDYdat, 
             type = "subnetworks", 
             subtype = 1, 
             layout_func = layout_in_circle,
             main = "Sub Network 1")
plotNetworks(TEDDYdat, 
             type = "subnetworks", 
             subtype = 2, 
             layout_func = layout_in_circle,
             main = "Sub Network 2")
```

<font size="3">**Visualizing Networks using Metscape or Cytoscape**</font>  


We made network visualization in third-party software easy by formatting the node and edge lists for input into Cytoscape or Metscape. You can save these tables as files using the `getNetworkFiles()` function. If no file path is provided, the two files save to the working directory.

```{r, getNetworkFiles, eval=FALSE}
#save files for cytoscape
getNetworkFiles(TEDDYdat)
```

Once the files are saved, open the Cytoscape software and read in the edge list. Go to file in the top left corner of Cytoscape, go down to import, and select Network From File . Select our edge list file <font color = red>*(1)*</font>.
```{r, import network, echo=FALSE}
knitr::include_graphics("./images/Cytoscape-importnetwork.jpg")
```
Next, click on the drop-down menu next to Metabolite A <font color = red>*(2)*</font> and select the green circle <font color = red>*(3)*</font>, making this the source node.
```{r, metabA image, echo=FALSE}
knitr::include_graphics("./images/Cytoscape-metabA.jpg")
```
Then click on the drop-down menu next to Metabolite B <font color = red>*(4)*</font> and select the red target <font color = red>*(5)*</font>, making this the target node. Hit the **OK** button in the bottom right corner of the pop-up <font color = red>*(6)*</font> to finish importing the network.
```{r, metabB image, echo=FALSE}
knitr::include_graphics("./images/Cytoscape-metabB.jpg")
```
Now that the edge list is imported, import the node list by selecting import table <font color = red>*(7)*</font> and select our node list file. 
```{r, nodetable, echo=FALSE}
knitr::include_graphics("./images/Cytoscape-nodetable.jpg")
```
Make sure the *Import Data as* drop-down says "Node Table Columns" <font color = red>*(8)*</font> and select **OK** in the bottom right corner of the pop-up <font color = red>*(9)*</font>.
```{r, nodetable confirm, echo=FALSE}
knitr::include_graphics("./images/cytoscape-nodetableconfirm.jpg")
```
Metscape is an app within Cytoscape that provides additional functionality for the visualization of metabolomics and lipidomics data. More information about these tools can be found on their respective websites linked at the top of [Network Visualization].

# References

